{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"hw2_shallow_and_deep_nws.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completing the notebook on Colab\n",
    "\n",
    "To complete the notebook on Colab, you can click on the \"Open in Colab\" button\n",
    "below and it will open the notebook from our public class notebook repository\n",
    "on GitHub.\n",
    "\n",
    "> Note that you will have save the notebook to your own Google Drive by clicking\n",
    "on File -> Save a Copy in Drive.\n",
    "\n",
    "> Also note that the Otter Grader cells will not run in Colab, but you can just\n",
    "> avoid executing those cells. For any public tests, you can submit to Gradescope\n",
    "> and inspect your results there.\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/DL4DS/sp2025_homeworks/blob/main/hw1/hw2_shallow_and_deep_nws.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completing the notebook in a local environment\n",
    "\n",
    "For local environments, after setting up your Python and Jupyter environments,\n",
    "you have the option of using either\n",
    "* Jupyter Lab or Notebook interface, or with\n",
    "* VSCode and the Jupyter extension pack\n",
    "\n",
    "In both cases we strongly encourage you to make sure you have a recent version\n",
    "of Python (>=3.10) and use a virtual environment such as `venv` or miniconda\n",
    "environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **HW2.1 -- Activation functions**\n",
    "\n",
    "The purpose of this practical is to experiment with different activation functions. <br>\n",
    "\n",
    "Work through the cells below, running each cell in turn. In various places you will see the words \"TO DO\". Follow the instructions at these places and write code to complete the functions. There are also questions interspersed in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports math library\n",
    "import numpy as np\n",
    "# Imports plotting library\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the shallow neural network.  We'll assume input in is range [0,1] and output [-1,1]\n",
    "# If the plot_all flag is set to true, then we'll plot all the intermediate stages as in Figure 3.3\n",
    "def plot_neural(x, y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3, plot_all=False, x_data=None, y_data=None):\n",
    "\n",
    "  # Plot intermediate plots if flag set\n",
    "  if plot_all:\n",
    "    fig, ax = plt.subplots(3,3)\n",
    "    fig.set_size_inches(8.5, 8.5)\n",
    "    fig.tight_layout(pad=3.0)\n",
    "    ax[0,0].plot(x,pre_1,'r-'); ax[0,0].set_ylabel('Preactivation')\n",
    "    ax[0,1].plot(x,pre_2,'b-'); ax[0,1].set_ylabel('Preactivation')\n",
    "    ax[0,2].plot(x,pre_3,'g-'); ax[0,2].set_ylabel('Preactivation')\n",
    "    ax[1,0].plot(x,act_1,'r-'); ax[1,0].set_ylabel('Activation')\n",
    "    ax[1,1].plot(x,act_2,'b-'); ax[1,1].set_ylabel('Activation')\n",
    "    ax[1,2].plot(x,act_3,'g-'); ax[1,2].set_ylabel('Activation')\n",
    "    ax[2,0].plot(x,w_act_1,'r-'); ax[2,0].set_ylabel('Weighted Act')\n",
    "    ax[2,1].plot(x,w_act_2,'b-'); ax[2,1].set_ylabel('Weighted Act')\n",
    "    ax[2,2].plot(x,w_act_3,'g-'); ax[2,2].set_ylabel('Weighted Act')\n",
    "\n",
    "    for plot_y in range(3):\n",
    "      for plot_x in range(3):\n",
    "        ax[plot_y,plot_x].set_xlim([0,1]);ax[plot_x,plot_y].set_ylim([-1,1])\n",
    "        ax[plot_y,plot_x].set_aspect(0.5)\n",
    "      ax[2,plot_y].set_xlabel('Input, $x$');\n",
    "    plt.show()\n",
    "\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.plot(x,y)\n",
    "  ax.set_xlabel('Input, $x$'); ax.set_ylabel('Output, $y$')\n",
    "  ax.set_xlim([0,1]);ax.set_ylim([-1,1])\n",
    "  ax.set_aspect(0.5)\n",
    "  if x_data is not None:\n",
    "    ax.plot(x_data, y_data, 'mo')\n",
    "    for i in range(len(x_data)):\n",
    "      ax.plot(x_data[i], y_data[i],)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a shallow neural network with, one input, one output, and three hidden units\n",
    "def shallow_1_1_3(x, activation_fn, phi_0,phi_1,phi_2,phi_3, theta_10, theta_11, theta_20, theta_21, theta_30, theta_31):\n",
    "  pre_1 = theta_10 + theta_11 * x\n",
    "  pre_2 = theta_20 + theta_21 * x\n",
    "  pre_3 = theta_30 + theta_31 * x\n",
    "  # Pass these through the ReLU function to compute the activations as in\n",
    "  # figure 3.3 d-f\n",
    "  act_1 = activation_fn(pre_1)\n",
    "  act_2 = activation_fn(pre_2)\n",
    "  act_3 = activation_fn(pre_3)\n",
    "\n",
    "  w_act_1 = phi_1 * act_1\n",
    "  w_act_2 = phi_2 * act_2\n",
    "  w_act_3 = phi_3 * act_3\n",
    "\n",
    "  y = phi_0 + w_act_1 + w_act_2 + w_act_3\n",
    "\n",
    "  # Return everything we have calculated\n",
    "  return y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Rectified Linear Unit (ReLU) function\n",
    "def ReLU(preactivation):\n",
    "    return np.maximum(0, preactivation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets define some parameters and run the neural network\n",
    "theta_10 =  0.3 ; theta_11 = -1.0\n",
    "theta_20 = -1.0  ; theta_21 = 2.0\n",
    "theta_30 = -0.5  ; theta_31 = 0.65\n",
    "phi_0 = -0.3; phi_1 = 2.0; phi_2 = -1.0; phi_3 = 7.0\n",
    "\n",
    "# Define a range of input values\n",
    "x = np.arange(0,1,0.01)\n",
    "\n",
    "# We run the neural network for each of these input values\n",
    "y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3 = \\\n",
    "    shallow_1_1_3(x, ReLU, phi_0,phi_1,phi_2,phi_3, theta_10, theta_11, theta_20, theta_21, theta_30, theta_31)\n",
    "# And then plot it\n",
    "plot_neural(x, y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3, plot_all=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sigmoid activation function\n",
    "\n",
    "The ReLU isn't the only kind of activation function.  For a long time, people used sigmoid functions.  A logistic sigmoid function is defined by the equation\n",
    "\n",
    "\\begin{equation}\n",
    "f[z] = \\frac{1}{1+\\exp{[-10 z ]}}\n",
    "\\end{equation}\n",
    "\n",
    "(Note that the factor of 10 is not standard -- but it allow us to plot on the same axes as the ReLU examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Define the sigmoid function\n",
    "def sigmoid(preactivation):\n",
    "  # TODO write code to implement the sigmoid function and compute the activation at the\n",
    "  # hidden unit from the preactivation.  Use the np.exp() function.\n",
    "  activation = ...\n",
    "\n",
    "  return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = np.linspace(-6, 6, 100)\n",
    "\n",
    "# Compute sigmoid values\n",
    "sigmoid_values = sigmoid(z)\n",
    "\n",
    "# Plot\n",
    "plt.plot(z, sigmoid_values, color='red')\n",
    "plt.xlabel(\"z\")\n",
    "plt.ylabel(\"sigmoid(z)\")\n",
    "plt.title(\"Sigmoid Function\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens when we use this activation function in a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_10 =  0.3 ; theta_11 = -1.0\n",
    "theta_20 = -1.0  ; theta_21 = 2.0\n",
    "theta_30 = -0.5  ; theta_31 = 0.65\n",
    "phi_0 = 0.3; phi_1 = 0.5; phi_2 = -1.0; phi_3 = 0.9\n",
    "\n",
    "# Define a range of input values\n",
    "x = np.arange(0,1,0.01)\n",
    "\n",
    "# We run the neural network for each of these input values\n",
    "y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3 = \\\n",
    "    shallow_1_1_3(x, sigmoid, phi_0,phi_1,phi_2,phi_3, theta_10, theta_11, theta_20, theta_21, theta_30, theta_31)\n",
    "# And then plot it\n",
    "plot_neural(x, y, pre_1, pre_2, pre_3, act_1, act_2, act_3, w_act_1, w_act_2, w_act_3, plot_all=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **HW2.2 -- Clipping functions**\n",
    "\n",
    "The purpose of the following cells is to understand how a neural network with two hidden layers build more complicated functions by clipping and recombining the representations at the intermediate hidden variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Rectified Linear Unit (ReLU) function\n",
    "def ReLU(preactivation):\n",
    "  activation = preactivation.clip(0.0)\n",
    "  return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def shallow_1_1_3_3(x, activation_fn, phi, psi, theta):\n",
    "\n",
    "    # Preactivations at layer 1 (terms in brackets in equation 4.7)\n",
    "    layer1_pre_1 = ...\n",
    "    layer1_pre_2 = ...\n",
    "    layer1_pre_3 = ...\n",
    "\n",
    "    # Activation functions (rest of equation 4.7)\n",
    "    h1 = activation_fn(layer1_pre_1)\n",
    "    h2 = activation_fn(layer1_pre_2)\n",
    "    h3 = activation_fn(layer1_pre_3)\n",
    "\n",
    "    # Preactivations at layer 2 (terms in brackets in equation 4.8)\n",
    "    layer2_pre_1 = ...\n",
    "    layer2_pre_2 = ...\n",
    "    layer2_pre_3 = ...\n",
    "\n",
    "    # Activation functions (rest of equation 4.8)\n",
    "    h1_prime = activation_fn(layer2_pre_1)\n",
    "    h2_prime = activation_fn(layer2_pre_2)\n",
    "    h3_prime = activation_fn(layer2_pre_3)\n",
    "\n",
    "    # Weighted outputs by phi (three last terms of equation 4.9)\n",
    "    phi1_h1_prime = ...\n",
    "    phi2_h2_prime = ...\n",
    "    phi3_h3_prime = ...\n",
    "\n",
    "    # Combine weighted activation and add y offset (summing terms of equation 4.9)\n",
    "    y = ...\n",
    "\n",
    "    # Return everything we have calculated\n",
    "    return y, layer2_pre_1, layer2_pre_2, layer2_pre_3, h1_prime, h2_prime, h3_prime, phi1_h1_prime, phi2_h2_prime, phi3_h3_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot two layer neural network as in figure 4.5\n",
    "def plot_neural_two_layers(x, y, layer2_pre_1, layer2_pre_2, layer2_pre_3, h1_prime, h2_prime, h3_prime, phi1_h1_prime, phi2_h2_prime, phi3_h3_prime):\n",
    "\n",
    "    fig, ax = plt.subplots(3,3)\n",
    "    fig.set_size_inches(8.5, 8.5)\n",
    "    fig.tight_layout(pad=3.0)\n",
    "    ax[0,0].plot(x,layer2_pre_1,'r-'); ax[0,0].set_ylabel(r'$\\psi_{10}+\\psi_{11}h_{1}+\\psi_{12}h_{2}+\\psi_{13}h_3$')\n",
    "    ax[0,1].plot(x,layer2_pre_2,'b-'); ax[0,1].set_ylabel(r'$\\psi_{20}+\\psi_{21}h_{1}+\\psi_{22}h_{2}+\\psi_{23}h_3$')\n",
    "    ax[0,2].plot(x,layer2_pre_3,'g-'); ax[0,2].set_ylabel(r'$\\psi_{30}+\\psi_{31}h_{1}+\\psi_{32}h_{2}+\\psi_{33}h_3$')\n",
    "    ax[1,0].plot(x,h1_prime,'r-'); ax[1,0].set_ylabel(r\"$h_{1}^{'}$\")\n",
    "    ax[1,1].plot(x,h2_prime,'b-'); ax[1,1].set_ylabel(r\"$h_{2}^{'}$\")\n",
    "    ax[1,2].plot(x,h3_prime,'g-'); ax[1,2].set_ylabel(r\"$h_{3}^{'}$\")\n",
    "    ax[2,0].plot(x,phi1_h1_prime,'r-'); ax[2,0].set_ylabel(r\"$\\phi_1 h_{1}^{'}$\")\n",
    "    ax[2,1].plot(x,phi2_h2_prime,'b-'); ax[2,1].set_ylabel(r\"$\\phi_2 h_{2}^{'}$\")\n",
    "    ax[2,2].plot(x,phi3_h3_prime,'g-'); ax[2,2].set_ylabel(r\"$\\phi_3 h_{3}^{'}$\")\n",
    "\n",
    "    for plot_y in range(3):\n",
    "      for plot_x in range(3):\n",
    "        ax[plot_y,plot_x].set_xlim([0,1]);ax[plot_x,plot_y].set_ylim([-1,1])\n",
    "        ax[plot_y,plot_x].set_aspect(0.5)\n",
    "      ax[2,plot_y].set_xlabel(r'Input, $x$');\n",
    "    plt.show()\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x,y)\n",
    "    ax.set_xlabel(r'Input, $x$'); ax.set_ylabel(r'Output, $y$')\n",
    "    ax.set_xlim([0,1]);ax.set_ylim([-1,1])\n",
    "    ax.set_aspect(0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define the parameters and visualize the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters (note first dimension of theta and phi is padded to make indices match\n",
    "# notation in book)\n",
    "theta = np.zeros([4,2])\n",
    "psi = np.zeros([4,4])\n",
    "phi = np.zeros([4,1])\n",
    "\n",
    "theta[1,0] =  0.3 ; theta[1,1] = -1.0\n",
    "theta[2,0]= -1.0  ; theta[2,1] = 2.0\n",
    "theta[3,0] = -0.5  ; theta[3,1] = 0.65\n",
    "psi[1,0] = 0.3;  psi[1,1] = 2.0; psi[1,2] = -1.0; psi[1,3]=7.0\n",
    "psi[2,0] = -0.2;  psi[2,1] = 2.0; psi[2,2] = 1.2; psi[2,3]=-8.0\n",
    "psi[3,0] = 0.3;  psi[3,1] = -2.3; psi[3,2] = -0.8; psi[3,3]=2.0\n",
    "phi[0] = 0.0; phi[1] = 0.5; phi[2] = -1.5; phi [3] = 2.2\n",
    "\n",
    "# Define a range of input values\n",
    "x = np.arange(0,1,0.01)\n",
    "\n",
    "# Run the neural network\n",
    "y, layer2_pre_1, layer2_pre_2, layer2_pre_3, h1_prime, h2_prime, h3_prime, phi1_h1_prime, phi2_h2_prime, phi3_h3_prime \\\n",
    "    = shallow_1_1_3_3(x, ReLU, phi, psi, theta)\n",
    "\n",
    "# And then plot it\n",
    "plot_neural_two_layers(x, y, layer2_pre_1, layer2_pre_2, layer2_pre_3, h1_prime, h2_prime, h3_prime, phi1_h1_prime, phi2_h2_prime, phi3_h3_prime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "To do:  To test your understanding of this, consider:\n",
    "\n",
    "1.   What would happen if we increase $\\psi_{1,0}$?\n",
    "2.   What would happen if we multiplied $\\psi_{2,0}, \\psi_{2,1}, \\psi_{2,2},  \\psi_{2,3}$ by -1?\n",
    "3.  What would happen if set $\\phi_{3}$ to -1?\n",
    "\n",
    "You can rerun the code to see if you were correct.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl4ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "otter": {
   "OK_FORMAT": true,
   "assignment_name": "hw2_shallow_and_deep_nws",
   "tests": {
    "q1": {
     "name": "q1",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert np.isclose(sigmoid(0), 0.5, atol=1e-07), 'Test case 1 failed: sigmoid(0) should be 0.5'\n>>> assert np.isclose(sigmoid(1), 1 / (1 + np.exp(-1)), atol=1e-07), 'Test case 2 failed: sigmoid(1) should be approximately 0.73105858'\n>>> assert np.isclose(sigmoid(-1), 1 / (1 + np.exp(1)), atol=1e-07), 'Test case 3 failed: sigmoid(-1) should be approximately 0.26894142'\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2": {
     "name": "q2",
     "points": 5,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> phi = np.array([0.5, -0.3, 0.8, 0.2])\n>>> psi = np.array([[0.2, -0.1, 0.3, 0.4], [0.4, 0.5, -0.2, -0.1], [-0.3, 0.7, 0.6, 0.2]])\n>>> theta = np.array([[0.1, -0.2], [0.3, 0.4], [-0.5, 0.6]])\n>>> x_test1 = np.array([0.0])\n>>> x_test2 = np.array([1.0])\n>>> x_test3 = np.array([-1.0])\n>>> (y1, l1p1_1, l1p2_1, l1p3_1, h1p1_1, h1p2_1, h1p3_1, phi1_h1p1, phi2_h2p1, phi3_h3p1) = shallow_1_1_3_3(x_test1, ReLU, phi, psi, theta)\n>>> (y2, l1p1_2, l1p2_2, l1p3_2, h1p1_2, h1p2_2, h1p3_2, phi1_h1p2, phi2_h2p2, phi3_h3p2) = shallow_1_1_3_3(x_test2, ReLU, phi, psi, theta)\n>>> (y3, l1p1_3, l1p2_3, l1p3_3, h1p1_3, h1p2_3, h1p3_3, phi1_h1p3, phi2_h2p3, phi3_h3p3) = shallow_1_1_3_3(x_test3, ReLU, phi, psi, theta)\n>>> assert np.isfinite(y1), 'Test 1 failed: Output should be finite for x=0'\n>>> assert y2 != y1, 'Test 2 failed: Output should change when x changes from 0 to 1'\n>>> assert y3 != y1, 'Test 3 failed: Output should change when x changes from 0 to -1'\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
